{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve most recent EOD positions from Quote Media and store in h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tickers...\n",
      "sending request https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA/TICKERS\n",
      "response <Response [200]> <bound method Response.json of <Response [200]>>\n",
      "Downloading data from https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/QUOTEMEDIA/TICKERS/QUOTEMEDIA_TICKERS_6d75499fefd916e54334b292986eafcc.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5PK74G73H%2F20241019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241019T163853Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCE9DHI6cxf91KCMK0bw6vq7zPkz658rsiQcjB1%2FStElAIgW4AHZkH32ZnjR1Sih5XGE%2FYn5Ak%2BgDXrjL1J0R%2F8TNAqjAUIYhACGgw1NDM2Mjk3NDIyMDIiDJyAt6zxe3Q2aE%2BxZCrpBL0ptFuyDey5r%2F9gqfoT2Y6tai52m6QrnhO3hdht%2Ftd%2BT0qr8YcxvSnweU4EsyNKA1epCOlvGQgp12MgFIrjqem0uPRvHlGKB4SoBX%2FFIVVVV8B%2BuXREkpryjH80PoJB8HwD2xHG%2BzJ5DT1vBDlnypFWxSzTRnwNOm%2B5Mm0P3hNRtB45DyZBgg65W5qE4ONRR6j%2FiRJBkAQa7gaUNDnbRgoMYJQNI9w8ZptL8bgENlpLlZFtG6CJXfJSuJoZ%2FECf5wXYLXkKTHaa2G9Tm5gnc1aB43QUbYbCLyTMB8KG3mn0l9NeVAr%2F4CYq6ubaEMv91bK7n1FbbjHuI5FWSny8ctsr6kMjbVtcu2zfgsMcSAv4D5nj6Fmzfrp4coIHi20CN0QCVmn01D8MAH2mgtPYXiRZLp0se4FRljSk2siPcxTqR01JxTAdeJ4sJohcDUavVvCQrDA8UnKS3Lx5RBnB3YMp%2FBG5I1xXtLmUi%2Fu4Y3oBhOCbOPz4%2FUKbvgWZLq%2B73q21M0JexIKzXmDJEYTSgffyrgyce6aHnVdy%2BLar%2FCxnElJPIBVMcLg4uywwP1q%2BJcCaeYgAJtODqW2Omh9uWQyZkzkAGXCBiCrUSjH%2FsQNxZvKSWMT8vk2Q0xSB%2F7FLdoHTXrLICSAVB3%2FtgVw23BqAtAFO7in35a21FpWnhgKNQKr7GEPNHig7camSXDAMKcu7ODNu719kjV54lVwKkGqhT4eSZUugMVFbFZKX%2FpZACp8okGMzVc9h8K%2FqmVHJYt8w6GpXaDGfY9HQsX2dT%2B95Uzxq6TBOx%2FOYj5wWvkP3%2BWrGeMZ%2Fanj3MJ3Bz7gGOpoBScnwkb1YBlL9CuiPYKGat2ii5vKL93wLxPv5Ry6V05IRoFITFATrmF%2F6cimH7iplyxJaPeBR8SeJzp1LEe%2B9MFjre5VgPMcoCDhcrE30IgNuPJPKjRMs6WNSnkUgp7il3NkpZ%2FsSFnz5%2Bwc8Agf1bX8lUJm2ici8CvFg803IwYnk8WiHLBveu44%2BNIFkZvt4gtk1ij1Zl30dHg%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=09ed8b6decf224a8be5faee04eebf273c339a96fdabbe93b8cf79a4a0da31c3f\n",
      "response:<Response [200]>\n",
      "Downloading adjusted EOD price data...\n",
      "sending request https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA/PRICES\n",
      "response <Response [200]> <bound method Response.json of <Response [200]>>\n",
      "Downloading data from https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/QUOTEMEDIA/PRICES/QUOTEMEDIA_PRICES_42bcf2b01643278cbecbc4a2ec871a5b.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5BSXQZQI6%2F20241019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241019T163910Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIC9%2F%2BUYXagTqkw5PAI0banHLvksV0q1b2787Q36aCEVvAiEA%2BP7EjMPIztnKxmbmNwKgh8Gs%2BrKw2txoNoItWPPc4NkqjAUIYhACGgw1NDM2Mjk3NDIyMDIiDP8q%2BXCOHeXbuflSByrpBCV0X1XJ4wAG9zsTi2T8ah9XcyU7HdTQj4YyNDgXbIixsfYxWJqAXl7mWchIXu01H97EYxt4p%2B58Z54bPz3rgzq0ctqoj7yjWA%2B5Lr1vZz8NgIsNB3wbHcVdUMZ1uoS02ULH0Kt30mAuesLHvaBrfUmAXlxRckc%2FSwqi7FywzrqbGA0%2FmfLdgJ7nWQFptX1oZ083R5sqA3KaGOsQfRSExADg%2FLzlXAr1VLtpsQzu2rPn3MVtEVc8xHlN0g3O3u9dY7eC%2Bb%2Fo2A3PIowiT3%2B58htlo4o43V3TaBPkrVhSy%2Fl9X0GmiscKNqlzhEUbgVyeezZSqlTw%2BzjfhYHjhV0s9CGBBPXsA7c6UmTi%2F0ISWx5Zd4cew2t5DOuf34l3lIGn38ThT1JL%2F868baca7xCmGfi7GCIgzzFaneM7iS%2FNqsTxKqEzKZ%2BXo0h8dWykqcr4PupPuGJn8yVBnAq%2BtjPC0pPNvJJyOq9Lp%2BH1IWC9C0diEr%2FDw70S3vJWj4T%2F8iXD5nRE5QzG6TSeJ2Dz7R5hCwfZ9d7TqwbG%2FHBXp8Tw6Ak0iKPUcbOoHHnkpRhnr5VXVocFXSkRkZRA6s2miHWorzznKfOL721JMbVePUnpMrffjaf%2FKrTst%2FU3YBCCCbTixI%2Fe90gPJb7wTCJLdtYklclHCRk3qDIQHcpXPyB4xJOrlkXV9pcZjnvxSWAtJS%2FEu2miAI%2FOnZChGd2TeO3odkmkMmSW%2Fck7XTbB5N5qElbPPVTKkEEGFk4%2FTRvsX1%2BXS%2FF0Jo%2Bw4%2Ba1hFvRgbqHlfTkOSfdbbmbAsFBqGnrHj18ak9HywDWPZnzMK7Bz7gGOpoB8qsky4qAAgbuoaneF7PZkSk%2BptLtlhZkdjLDoEEAxwBjbWRAy7%2BLe4tpMT9rIEZxqh%2BYbQm%2FY5l%2FzPVt0uMCG5bJI3fluL%2FDyptgcldMoJm1XOKu9G5h8yKlGsBA1EKkHmLnCjaRZt4Sbvi80cToWGL8mVF%2FCmC9HraFwMnu6nGzOZcglC9sqXJNp4OVDIqoXptDrqTG%2FGnnew%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=f1d0ff19e065e96aac7189d871da3c0c207d4939b92a04095f8699eaad407ce7\n",
      "response:<Response [200]>\n",
      "Storing data in H5 format...\n",
      "Data successfully stored in quotemedia_eod_data.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "\n",
    "# Set up API key and base URL\n",
    "API_KEY = \"tw2sxkKZo_y1UvMcnSux\"\n",
    "BASE_URL = \"https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA\"\n",
    "\n",
    "# Function to download data from API\n",
    "def get_data(endpoint, params):\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    params[\"api_key\"] = API_KEY\n",
    "    print(f\"sending request {url}\")\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"response {response} {response.json}\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "# Function to download and process ZIP file\n",
    "def download_and_process_zip(url):\n",
    "    print(f\"Downloading data from {url}\")\n",
    "    response = requests.get(url)\n",
    "    print(f'response:{response}')\n",
    "    if response.status_code == 200:\n",
    "        z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        csv_filename = z.namelist()[0]  # Assume the first file in the ZIP is the CSV we want\n",
    "        with z.open(csv_filename) as f:\n",
    "            df = pd.read_csv(f)\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
    "\n",
    "# Download tickers\n",
    "print(\"Downloading tickers...\")\n",
    "tickers_response = get_data(\"TICKERS\", {\"qopts.export\": \"true\"})\n",
    "tickers_download_link = tickers_response['datatable_bulk_download']['file']['link']\n",
    "tickers_df = download_and_process_zip(tickers_download_link)\n",
    "\n",
    "# Download adjusted EOD price data\n",
    "print(\"Downloading adjusted EOD price data...\")\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {\n",
    "    \"date.gte\": start_date,\n",
    "    \"date.lte\": end_date,\n",
    "    \"qopts.columns\": \"ticker,date,adj_open,adj_high,adj_low,adj_close,adj_volume\",\n",
    "    \"qopts.export\": \"true\"\n",
    "}\n",
    "time.sleep(10)\n",
    "price_response = get_data(\"PRICES\", params)\n",
    "time.sleep(10)\n",
    "price_download_link = price_response['datatable_bulk_download']['file']['link']\n",
    "prices_df = download_and_process_zip(price_download_link)\n",
    "\n",
    "# Convert date column to datetime\n",
    "prices_df[\"date\"] = pd.to_datetime(prices_df[\"date\"])\n",
    "prices_df.rename(columns={\"adj_open\":'open','adj_high':'high','adj_low':'low','adj_close':'close','adj_volume':'volume'},inplace=True)\n",
    "# Store data in H5 format\n",
    "print(\"Storing data in H5 format...\")\n",
    "with h5py.File(\"quotemedia_eod_data.h5\", \"w\") as f:\n",
    "    # Store tickers data\n",
    "    tickers_group = f.create_group(\"tickers\")\n",
    "    for column in tickers_df.columns:\n",
    "        if tickers_df[column].dtype == 'object':\n",
    "            # Convert string columns to ASCII\n",
    "            ascii_values = [s.encode('ascii', 'ignore') if isinstance(s, str) else b'' for s in tickers_df[column].values]\n",
    "            tickers_group.create_dataset(column, data=ascii_values, dtype=h5py.special_dtype(vlen=bytes))\n",
    "        else:\n",
    "            tickers_group.create_dataset(column, data=tickers_df[column].values)\n",
    "    \n",
    "    # Store prices data\n",
    "    prices_group = f.create_group(\"prices\")\n",
    "    for column in prices_df.columns:\n",
    "        if column == \"date\":\n",
    "            prices_group.create_dataset(column, data=prices_df[column].astype(int))\n",
    "        elif prices_df[column].dtype == 'object':\n",
    "            # Convert string columns to ASCII\n",
    "            ascii_values = [s.encode('ascii', 'ignore') if isinstance(s, str) else b'' for s in prices_df[column].values]\n",
    "            prices_group.create_dataset(column, data=ascii_values, dtype=h5py.special_dtype(vlen=bytes))\n",
    "        else:\n",
    "            prices_group.create_dataset(column, data=prices_df[column].values)\n",
    "\n",
    "print(\"Data successfully stored in quotemedia_eod_data.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tickers...\n",
      "sending request https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA/TICKERS\n",
      "response <Response [200]> <bound method Response.json of <Response [200]>>\n",
      "Downloading data from https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/QUOTEMEDIA/TICKERS/QUOTEMEDIA_TICKERS_6d75499fefd916e54334b292986eafcc.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5HPLKQTK4%2F20241019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241019T201545Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEP3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD12fz821%2BOdDSgeZugMSPmJCGxV87hfTQjzmP1x7u%2BYAIgV2qaw5VUWUj%2Flhy2vZkN2aVRSC7x2vlUyrXeRhmSFbEqjAUIZRACGgw1NDM2Mjk3NDIyMDIiDKCjHdJ6rcVftPonGCrpBAH%2FIH7VFBXSi2xbJEIe%2F9tVkhJ6rPd4zCi8VV8xHqMUSTGNGT%2B%2Bw1hJwUBorVjcECOak1e2NvRDGCk9EWQcFknM0Ju5c82O1idKY1FxhmQRBmmjpK3r%2BA7TR%2FOWbTcSXUwG%2FgjruPDHmyZzrcUnCET9s2%2FckqO07bKc206BbgqTefEqKRSnE6H3ub%2F%2BYj9q6PLaLYdlyq5R9EsdLa7TLcddfP2gpvW1G0sEStLM4cUboCrKoG1Gz5E4thJBC9Zcn%2BKluHJTdGh1rT2pRhKClIPzJ77On5q6cOt0J28UPMBsOQRLiKOV%2BxtQByU4jkGqAa10grm2spIEJ9HQQleOipyYjuvRME%2FVyA%2BihwhtXPM1dkDmbQeJcRXxxpqdMSqcRHSah5eg7nn8sQQdHxnUUnub3mwnEmEcoAuZS11cxXOUWP8y50tbpPtyuXzoamDlKYOzwdywJ3ygprXBrolOTY3v3CH1zkTAPYObfmvTJr%2FwOobX6VFT1%2ByboJm%2Bs7kYvvjrijFZmqXOhrH43cERqeygmjtoyD4GGX40h4Dh2tO3w4ixtCI4AemtIQBXwQbp%2FDqoziBMmP%2BRDlHFYIUgD7yfR4kxLraKAdQy5mcx%2FgaY0nD6JbfhX6akDXDgO7Zd81v7487QMmHIw1FJiwiMvYpmMpZlkRtgkKo1slwe%2FDJcner7TMATYlNOW7QnPIvyQLSoWTt%2B57HS2%2F4UGBcoaOaUHgKwCXT%2BgPX1AP3lZlpDXTvT7v2dChOyU9mdhP8ePMN4Csqd%2FlAecXqMo1p6xd6s%2Bl6747b851UZqHdqz7BiwCm4iDz39jGZMPGm0LgGOpoB8K8W7cKrZrF3tvpOhjdu3HDhbyULUP%2FDpm5dUo9qt3pxUkSz8lLAC70h5gMaPzI3axbGwoM19Mzr50BKuP4prxxAOPWwv823S7tGHjq11I89PLZCEIIneKNlCv13OM2aQjNdX6F%2FQ9DEwkY2VKOx0s0bk5qjAL1q99sTvKh4brQwm4xGibrYj%2Fx%2FNqsSl1mrSKgldGAXyLt7jw%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=6f88b54fd9c896fe2b8fbd91133f1fc806ad599d0af215c8afd93a81c641910b\n",
      "response:<Response [200]>\n",
      "Downloading adjusted EOD price data...\n",
      "sending request https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA/PRICES\n",
      "response <Response [200]> <bound method Response.json of <Response [200]>>\n"
     ]
    }
   ],
   "source": [
    "# New Download code that will store the data using the pandas library\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up API key and base URL\n",
    "API_KEY = \"tw2sxkKZo_y1UvMcnSux\"\n",
    "BASE_URL = \"https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA\"\n",
    "\n",
    "# Function to download data from API\n",
    "def get_data(endpoint, params):\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    params[\"api_key\"] = API_KEY\n",
    "    print(f\"sending request {url}\")\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"response {response} {response.json}\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "# Function to download and process ZIP file\n",
    "def download_and_process_zip(url):\n",
    "    print(f\"Downloading data from {url}\")\n",
    "    response = requests.get(url)\n",
    "    print(f'response:{response}')\n",
    "    if response.status_code == 200:\n",
    "        z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        csv_filename = z.namelist()[0]  # Assume the first file in the ZIP is the CSV we want\n",
    "        with z.open(csv_filename) as f:\n",
    "            df = pd.read_csv(f)\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
    "\n",
    "# Download tickers\n",
    "print(\"Downloading tickers...\")\n",
    "tickers_response = get_data(\"TICKERS\", {\"qopts.export\": \"true\"})\n",
    "tickers_download_link = tickers_response['datatable_bulk_download']['file']['link']\n",
    "tickers_df = download_and_process_zip(tickers_download_link)\n",
    "\n",
    "# Download adjusted EOD price data\n",
    "print(\"Downloading adjusted EOD price data...\")\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {\n",
    "    \"date.gte\": start_date,\n",
    "    \"date.lte\": end_date,\n",
    "    \"qopts.columns\": \"ticker,date,adj_open,adj_high,adj_low,adj_close,adj_volume\",\n",
    "    \"qopts.export\": \"true\"\n",
    "}\n",
    "time.sleep(10)\n",
    "price_response = get_data(\"PRICES\", params)\n",
    "time.sleep(10)\n",
    "price_download_link = price_response['datatable_bulk_download']['file']['link']\n",
    "prices_df = download_and_process_zip(price_download_link)\n",
    "\n",
    "# Convert date column to datetime and rename columns\n",
    "prices_df[\"date\"] = pd.to_datetime(prices_df[\"date\"])\n",
    "prices_df.rename(columns={\"adj_open\": 'open', 'adj_high': 'high', 'adj_low': 'low', 'adj_close': 'close', 'adj_volume': 'volume'}, inplace=True)\n",
    "\n",
    "# Store data in H5 format using pandas\n",
    "print(\"Storing data in H5 format using pandas...\")\n",
    "h5_path = \"quotemedia_eod_data.h5\"\n",
    "\n",
    "# Writing the tickers and prices DataFrames into the HDF5 file\n",
    "with pd.HDFStore(h5_path, mode='w') as store:\n",
    "    # Store the tickers data with a key for easy access\n",
    "    store.put('tickers', tickers_df, format='table')\n",
    "    \n",
    "    # Store the prices data with a key for easy access\n",
    "    store.put('prices', prices_df, format='table')\n",
    "\n",
    "print(\"Data successfully stored in quotemedia_eod_data.h5 using pandas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading adjusted EOD price data...\n",
      "sending request https://data.nasdaq.com/api/v3/datatables/QUOTEMEDIA/PRICES\n",
      "response <Response [200]> <bound method Response.json of <Response [200]>>\n",
      "API Response: {'datatable_bulk_download': {'file': {'link': 'https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/QUOTEMEDIA/PRICES/QUOTEMEDIA_PRICES_42bcf2b01643278cbecbc4a2ec871a5b.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5GVCTFUY5%2F20241019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241019T162550Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCtsP39eClUZkM%2BAbPlMIk5xl23BCk6PuXpDSblwKBIJgIgJHWOtWssr3cX%2Fws2Fw7TdDIu424vYRJ35yipkMXzdywqjAUIYhACGgw1NDM2Mjk3NDIyMDIiDPrHEZ3u9d5nZJxCpCrpBLF9h9by48ispCGd36U6p2%2BNKV%2FY2F0pjkZEkke%2FywktZCBEWotmGOCQAW%2BgrdLhPiVL3zmHXLleQKt7T%2FJ9xFDuJEERjQRxZO9PsH6Z00VPwAyqjlKs0ObNrpFb0HecpieYvlREZujCd1Ip7UG61ZkBAl4iLkuvktASetUcrhwcyclir28vcM0XlLI%2FRTFqm1las11Fs9adVnbaw7FlpZWZ7jKut8ns4X8oKnYljY7B26dMJ2PTWgGafnFpoyv%2B7IQbhIM5kZB4jRt%2Br0%2F5h4Y%2BqYWGfR%2FX9oBIZccQFWAthX05cOiU1lrsSBLhmkjVd%2FlC7F9dMkXeXbk3xcVy9mxSH%2FcPD%2BTKE95Rrb0eTBkNh3qE2Xq0Nktcbmx3Vv3ZXWqYM0jy16jAFgGvKSM31xYMs2OdKdsCJus2HlGSot9od2nZgVuwc7U3tPRlt3MwLVhM%2FSZic161881yzzeNTJl119kFu5okoy9KIVqlddKRP87YLTh564CJew7xw8bnbdofyjCP4hdQ3Sj1zKWRap6bhExGF0X0fk46e7WbPbQxCTN3L1qf6y0w%2Fx1zdBYHCoWQfAlZ3eTgIFCaFpqyk33greeMLOzn2roPQFAww32Qt4ZM%2Fc2u3jWC%2FevxRBYB0r7%2F8KPiz2MkI%2BcgNRrkyIsA4olLFsabm91lVXCUF5zBp%2BhLwVwprQrkyW3XK0XhlGRmAzTUibNheBuh8ti%2BzH3J%2F4vC2SoJ8GeqR3LXUJ9H6Y3J9Fdug8GjahJEnqeNq8j40ykBL7wGqMoVUK7CvBNfHyHXQCQ5t0%2BHCMcF5CUXVs1Awg1s1didMI67z7gGOpoBEQY12tAhm0lnpXEYeZ4TELPXtmn3A%2F42sDAWIPXFws2NBQdmDehrMHBh3Iw2JWGrR%2FRYOcBnAeJTpI3jEVrNRhwkXcr00n3fUnZaYoh8%2FanGD%2B1d4ZbIQibv4JwS3g2BaJ6br60cqoKl5xfGLe%2BaEQZZdkuijzC%2FYAgTNVPYgCA3Z%2F1Q37B6x%2BdDWuTudG8OWT0qJvEDVVi7WQ%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=a18d3290f8b8492d5fa3158f0ffff0fcac045edeb513e495601ca9eeae7aab85', 'status': 'fresh', 'data_snapshot_time': '2024-10-19 15:33:34 UTC'}, 'datatable': {'last_refreshed_time': '2024-10-19 00:12:43 UTC'}}}\n",
      "Downloading data from https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/QUOTEMEDIA/PRICES/QUOTEMEDIA_PRICES_42bcf2b01643278cbecbc4a2ec871a5b.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5GVCTFUY5%2F20241019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241019T162550Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCtsP39eClUZkM%2BAbPlMIk5xl23BCk6PuXpDSblwKBIJgIgJHWOtWssr3cX%2Fws2Fw7TdDIu424vYRJ35yipkMXzdywqjAUIYhACGgw1NDM2Mjk3NDIyMDIiDPrHEZ3u9d5nZJxCpCrpBLF9h9by48ispCGd36U6p2%2BNKV%2FY2F0pjkZEkke%2FywktZCBEWotmGOCQAW%2BgrdLhPiVL3zmHXLleQKt7T%2FJ9xFDuJEERjQRxZO9PsH6Z00VPwAyqjlKs0ObNrpFb0HecpieYvlREZujCd1Ip7UG61ZkBAl4iLkuvktASetUcrhwcyclir28vcM0XlLI%2FRTFqm1las11Fs9adVnbaw7FlpZWZ7jKut8ns4X8oKnYljY7B26dMJ2PTWgGafnFpoyv%2B7IQbhIM5kZB4jRt%2Br0%2F5h4Y%2BqYWGfR%2FX9oBIZccQFWAthX05cOiU1lrsSBLhmkjVd%2FlC7F9dMkXeXbk3xcVy9mxSH%2FcPD%2BTKE95Rrb0eTBkNh3qE2Xq0Nktcbmx3Vv3ZXWqYM0jy16jAFgGvKSM31xYMs2OdKdsCJus2HlGSot9od2nZgVuwc7U3tPRlt3MwLVhM%2FSZic161881yzzeNTJl119kFu5okoy9KIVqlddKRP87YLTh564CJew7xw8bnbdofyjCP4hdQ3Sj1zKWRap6bhExGF0X0fk46e7WbPbQxCTN3L1qf6y0w%2Fx1zdBYHCoWQfAlZ3eTgIFCaFpqyk33greeMLOzn2roPQFAww32Qt4ZM%2Fc2u3jWC%2FevxRBYB0r7%2F8KPiz2MkI%2BcgNRrkyIsA4olLFsabm91lVXCUF5zBp%2BhLwVwprQrkyW3XK0XhlGRmAzTUibNheBuh8ti%2BzH3J%2F4vC2SoJ8GeqR3LXUJ9H6Y3J9Fdug8GjahJEnqeNq8j40ykBL7wGqMoVUK7CvBNfHyHXQCQ5t0%2BHCMcF5CUXVs1Awg1s1didMI67z7gGOpoBEQY12tAhm0lnpXEYeZ4TELPXtmn3A%2F42sDAWIPXFws2NBQdmDehrMHBh3Iw2JWGrR%2FRYOcBnAeJTpI3jEVrNRhwkXcr00n3fUnZaYoh8%2FanGD%2B1d4ZbIQibv4JwS3g2BaJ6br60cqoKl5xfGLe%2BaEQZZdkuijzC%2FYAgTNVPYgCA3Z%2F1Q37B6x%2BdDWuTudG8OWT0qJvEDVVi7WQ%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=a18d3290f8b8492d5fa3158f0ffff0fcac045edeb513e495601ca9eeae7aab85\n",
      "response:<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Download adjusted EOD price data\n",
    "print(\"Downloading adjusted EOD price data...\")\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {\n",
    "    \"date.gte\": start_date,\n",
    "    \"date.lte\": end_date,\n",
    "    \"qopts.columns\": \"ticker,date,adj_open,adj_high,adj_low,adj_close,adj_volume\",\n",
    "    \"qopts.export\": \"true\"\n",
    "}\n",
    "\n",
    "price_response = get_data(\"PRICES\", params)\n",
    "print(f\"API Response: {price_response}\")\n",
    "time.sleep(10)\n",
    "if 'datatable_bulk_download' in price_response and 'file' in price_response['datatable_bulk_download']:\n",
    "    price_download_link = price_response['datatable_bulk_download']['file'].get('link')\n",
    "    if price_download_link:\n",
    "        prices_df = download_and_process_zip(price_download_link)\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        prices_df[\"date\"] = pd.to_datetime(prices_df[\"date\"])\n",
    "        prices_df.rename(columns={\"adj_open\":'open','adj_high':'high','adj_low':'low','adj_close':'close','adj_volume':'volume'}, inplace=True)\n",
    "        \n",
    "        # Continue with the rest of your code...\n",
    "    else:\n",
    "        print(\"Error: No download link found in the API response.\")\n",
    "else:\n",
    "    print(\"Error: Unexpected API response structure.\")\n",
    "    print(f\"Response content: {price_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JTKWY</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>6.1700</td>\n",
       "      <td>7.3200</td>\n",
       "      <td>5.790</td>\n",
       "      <td>6.72</td>\n",
       "      <td>9440097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JTKWY</td>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>6.1600</td>\n",
       "      <td>6.1750</td>\n",
       "      <td>5.935</td>\n",
       "      <td>6.07</td>\n",
       "      <td>2261623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FG_1</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>8.1000</td>\n",
       "      <td>8.3900</td>\n",
       "      <td>8.100</td>\n",
       "      <td>8.39</td>\n",
       "      <td>3086317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLWS</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>14.5700</td>\n",
       "      <td>14.9588</td>\n",
       "      <td>14.410</td>\n",
       "      <td>14.45</td>\n",
       "      <td>662492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RENW_</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>21.9768</td>\n",
       "      <td>21.9900</td>\n",
       "      <td>21.970</td>\n",
       "      <td>21.99</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44330218</th>\n",
       "      <td>CCIRU</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>9.9800</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>9.980</td>\n",
       "      <td>10.00</td>\n",
       "      <td>96979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44330219</th>\n",
       "      <td>NATO</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>25.6000</td>\n",
       "      <td>25.6000</td>\n",
       "      <td>25.580</td>\n",
       "      <td>25.58</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44330220</th>\n",
       "      <td>RDACU</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>10.0200</td>\n",
       "      <td>10.010</td>\n",
       "      <td>10.02</td>\n",
       "      <td>13427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44330221</th>\n",
       "      <td>STFS</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>2.990</td>\n",
       "      <td>3.02</td>\n",
       "      <td>175495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44330222</th>\n",
       "      <td>UPB</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>23.0600</td>\n",
       "      <td>23.6500</td>\n",
       "      <td>21.950</td>\n",
       "      <td>23.00</td>\n",
       "      <td>242502.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44330223 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ticker       date     open     high     low  close     volume\n",
       "0         JTKWY 2022-03-11   6.1700   7.3200   5.790   6.72  9440097.0\n",
       "1         JTKWY 2022-03-10   6.1600   6.1750   5.935   6.07  2261623.0\n",
       "2          FG_1 2020-06-01   8.1000   8.3900   8.100   8.39  3086317.0\n",
       "3          FLWS 2022-03-09  14.5700  14.9588  14.410  14.45   662492.0\n",
       "4         RENW_ 2020-01-29  21.9768  21.9900  21.970  21.99      319.0\n",
       "...         ...        ...      ...      ...     ...    ...        ...\n",
       "44330218  CCIRU 2024-10-15   9.9800  10.0000   9.980  10.00    96979.0\n",
       "44330219   NATO 2024-10-15  25.6000  25.6000  25.580  25.58      123.0\n",
       "44330220  RDACU 2024-10-15  10.0100  10.0200  10.010  10.02    13427.0\n",
       "44330221   STFS 2024-10-15   3.5400   3.5400   2.990   3.02   175495.0\n",
       "44330222    UPB 2024-10-15  23.0600  23.6500  21.950  23.00   242502.0\n",
       "\n",
       "[44330223 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIOA_WS</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>BioAmber Inc. Warrant expiring May 9 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CDE_WS</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>Coeur D'Alene Mines Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FINQ</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Purefunds Solactive FinTech ETF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMED</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>PureFunds ETFx HealthTech ETF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVETV</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Covetrus Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22981</th>\n",
       "      <td>ATAKR</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Aurora Technology Acquisition Corp. Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22982</th>\n",
       "      <td>MURF</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Conduit Pharmaceuticals Inc Com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22983</th>\n",
       "      <td>RACY</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Relativity Acquisition Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22984</th>\n",
       "      <td>ACAX</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Alset Capital Acquisition Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22985</th>\n",
       "      <td>AXAC_R</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>AXIOS Sustainable Growth Acquisition Corporati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22986 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker exchange                                       company_name\n",
       "0      BIOA_WS     NYSE          BioAmber Inc. Warrant expiring May 9 2017\n",
       "1       CDE_WS     NYSE                    Coeur D'Alene Mines Corporation\n",
       "2         FINQ   NASDAQ                    Purefunds Solactive FinTech ETF\n",
       "3         IMED   NASDAQ                      PureFunds ETFx HealthTech ETF\n",
       "4        CVETV   NASDAQ                                      Covetrus Inc.\n",
       "...        ...      ...                                                ...\n",
       "22981    ATAKR   NASDAQ         Aurora Technology Acquisition Corp. Rights\n",
       "22982     MURF   NASDAQ                    Conduit Pharmaceuticals Inc Com\n",
       "22983     RACY   NASDAQ                       Relativity Acquisition Corp.\n",
       "22984     ACAX   NASDAQ                    Alset Capital Acquisition Corp.\n",
       "22985   AXAC_R     NYSE  AXIOS Sustainable Growth Acquisition Corporati...\n",
       "\n",
       "[22986 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_path /home/morgan/repos/edge-seeker/.zipline/custom_data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m custom_data_path \u001b[38;5;241m=\u001b[39m Path(zipline_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquotemedia_eod_data.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zip312/lib/python3.12/site-packages/pandas/io/pytables.py:435\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m groups \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mgroups()\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset(s) incompatible with Pandas data types, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot table, or no datasets found in HDF5 file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m candidate_only_group \u001b[38;5;241m=\u001b[39m groups[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# For the HDF file to have only one dataset, all other groups\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# should then be metadata groups for that candidate group. (This\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# assumes that the groups() method enumerates parent groups\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# before their children.)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file."
     ]
    }
   ],
   "source": [
    "#/home/morgan/repos/edge-seeker/.zipline/custom_data/quotemedia_eod_data.h5\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "zipline_root = os.path.expanduser('~/repos/edge-seeker/.zipline')\n",
    "custom_data_path = Path(zipline_root, 'custom_data')\n",
    "print(f'custom_path {custom_data_path}')\n",
    "pd.read_hdf(custom_data_path / 'quotemedia_eod_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot create a storer if the object is not existing nor a value are passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(h5_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5_file:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m h5_file:\n\u001b[0;32m----> 7\u001b[0m         equities_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(equities_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Check if the dataset has the correct structure\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/zipline_x_312/lib/python3.12/site-packages/pandas/io/pytables.py:452\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/zipline_x_312/lib/python3.12/site-packages/pandas/io/pytables.py:885\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# create the storer and axes\u001b[39;00m\n\u001b[1;32m    884\u001b[0m where \u001b[38;5;241m=\u001b[39m _ensure_term(where, scope_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 885\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_storer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m s\u001b[38;5;241m.\u001b[39minfer_axes()\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# function to call on iteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/zipline_x_312/lib/python3.12/site-packages/pandas/io/pytables.py:1752\u001b[0m, in \u001b[0;36mHDFStore._create_storer\u001b[0;34m(self, group, format, value, encoding, errors)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         tt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric_table\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1752\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot create a storer if the object is not existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1754\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnor a value are passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1755\u001b[0m         )\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot create a storer if the object is not existing nor a value are passed"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "h5_path = 'quotemedia_eod_data.h5'\n",
    "with h5py.File(h5_path, 'r') as h5_file:\n",
    "    if 'prices' in h5_file:\n",
    "        equities_df = pd.read_hdf(h5_path, 'prices')\n",
    "        print(equities_df.head())\n",
    "        # Check if the dataset has the correct structure\n",
    "        required_columns = ['sid', 'symbol', 'exchange', 'asset_name']\n",
    "        missing_columns = [col for col in required_columns if col not in equities_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns in equities dataset: {missing_columns}\")\n",
    "        else:\n",
    "            print(\"Equities dataset format is correct.\")\n",
    "    else:\n",
    "        print(\"Equities dataset not found in the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zipline_x_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
